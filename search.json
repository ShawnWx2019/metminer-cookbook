[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MetMiner cookbook",
    "section": "",
    "text": "Project\nMetMiner: A user-friendly pipeline for large-scale plant metabolomics data analysis.\nThis is the documentation of MetMiner. You can get up-to-date verison from the GitHub repository https://github.com/ShawnWx2019/MetMiner.\nIn this pipeline, tidyMass Shen et al. (2022) take on the majority of the upstream data analysis, data cleaning, and metabolite annotation tasks.\nFurthermore, we have developed a Metabolomics Downstream Analysis toolkit MDAtoolkits and a user-friendly WGCNA Shiny app. They are responsible for the downstream analysis and data mining section.\nThe metMiner shiny app has been packaged into a TBtools Chen et al. (2023) plugin, which can be downloaded and installed through the TBtools plugin store. Thanks to TBtools for providing a convenient dependency resolution solution.\n\n\nCitation\nIf you have utilized MetMiner in your project, please cite:\n\nXiao Wang, Shuang Liang, Ke Yu, Wenqi Yang, Bing Zhao, Fei Liang, Xiang Zhu, Chao Zhou, Luis A. J. Mur, Jeremy A, Roberts, Junli Zhang,and Xuebin Zhang. 2024 “MetMiner: A user-friendly pipeline for large-scale plant metabolomics data analysis”\nShen, Xiaotao, Hong Yan, Chuchu Wang, Peng Gao, Caroline H. Johnson, and Michael P. Snyder. 2022. “TidyMass an Object-Oriented Reproducible Analysis Framework for LC Data.” Nature Communications 13 (1): 4365. https://doi.org/10.1038/s41467-022-32155-w.\n\n\n\n\n\n\nReferences\n\nChen, Chengjie, Ya Wu, Jiawei Li, Xiao Wang, Zaohai Zeng, Jing Xu, Yuanlong Liu, et al. 2023. “TBtools-II: A \"One for All, All for One\" Bioinformatics Platform for Biological Big-Data Mining.” Molecular Plant 0 (0). https://doi.org/10.1016/j.molp.2023.09.010.\n\n\nShen, Xiaotao, Hong Yan, Chuchu Wang, Peng Gao, Caroline H. Johnson, and Michael P. Snyder. 2022. “TidyMass an Object-Oriented Reproducible Analysis Framework for LC–MS Data.” Nature Communications 13 (1): 4365. https://doi.org/10.1038/s41467-022-32155-w.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "content/Datamining_method.html",
    "href": "content/Datamining_method.html",
    "title": "MetMiner Cookbook",
    "section": "",
    "text": "4.3 Data integration\nSet the method for integrating the data, then click the Start integration button to proceed with data integration. The result will generate an accumulation matrix, sample information, variable information, and integrated results of compound annotation, which you can download.\n\nSwitch to the Figures tab, click the Start visualization button, adjust PCA grouping colors, and proceed with plotting. You can also set the image size, then click Download to download the image. Open the Interactive plot button, where you can view the 3D PCA plot, hover the mouse to see detailed sample information. Scroll down the page, at the bottom, you can see the sample correlation plot. You can adjust its colors, grouping, clustering, and more.",
    "crumbs": [
      "Data mining",
      "Introduction"
    ]
  },
  {
    "objectID": "content/references.html",
    "href": "content/references.html",
    "title": "References",
    "section": "",
    "text": "References",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "content/enrich.html",
    "href": "content/enrich.html",
    "title": "Enrichment",
    "section": "",
    "text": "Enrichment\nFirst, in the Data_Export folder, locate the table_merge.xlsx file and save the sheet named annotation_table as a CSV file to use it as the annotation file. Click ✓Start, switch to the Table tab, enter the metabolite ID, and click Start to obtain the Enrichment table. Switch to the Plot tab to visualize the enrichment results.",
    "crumbs": [
      "Downstream data processing",
      "Enrichment analysis"
    ]
  },
  {
    "objectID": "content/feature_dedup.html",
    "href": "content/feature_dedup.html",
    "title": "Feature deduplication",
    "section": "",
    "text": "Feature deduplication\nFirst, select hydrophilic interaction chromatography (HILIC) or reverse phase chromatography (RP), then choose the ion mode, and finally select the filtering method for annotation. For multiple annotations: you can choose to retain all, retain those with high scores, or retain the first annotation. For removing redundant annotations: you can choose to retain all or retain the first annotation. You can also filter annotations based on accuracy: you can choose to retain all or those with higher accuracy (1,2). After the parameters are set, click the Start annotation filtering button to filter annotations.\n\n\nData integration\nSet the method for integrating the data, then click the Start integration button to proceed with data integration. The result will generate an accumulation matrix, sample information, variable information, and integrated results of compound annotation, which you can download.\n\nSwitch to the Figures tab, click the Start visualization button, adjust PCA grouping colors, and proceed with plotting. You can also set the image size, then click Download to download the image. Open the Interactive plot button, where you can view the 3D PCA plot, hover the mouse to see detailed sample information. Scroll down the page, at the bottom, you can see the sample correlation plot. You can adjust its colors, grouping, clustering, and more.",
    "crumbs": [
      "Downstream data processing",
      "Feature deduplication"
    ]
  },
  {
    "objectID": "content/iWGCNA.html#expression-matrix",
    "href": "content/iWGCNA.html#expression-matrix",
    "title": "Data mining by iWGCNA",
    "section": "Expression matrix",
    "text": "Expression matrix\nYou can prepare your datExpr file following the Expression matrix flie\nData source:\n\ntranscriptomics\n\nreadcount.\nexpected count\nnormalized readcount (FPKM, RPKM, TPM, CPM)\nmicroarray data\n\nmetabolomics\n\npeak area.\n\nproteomics,\n\nprotein abundance.\ncorrected intensity\n\n…\n\nFormat:\n\nGene/metabolite/protein ID in row and sample ID in column.\nThe sample ID should not contain spaces (), special symbols (- @ * & #) etc., and should not start with numbers.\nDO NOT use pure numbers as gene/metabolite/protein ID.\nOnly accepted tab-delimited file, such as .txt or .tsv, DO NOT use .csv, .xlsx, .xls.",
    "crumbs": [
      "Data mining",
      "iterative WGCNA"
    ]
  },
  {
    "objectID": "content/iWGCNA.html#trait-table.",
    "href": "content/iWGCNA.html#trait-table.",
    "title": "Data mining by iWGCNA",
    "section": "Trait table.",
    "text": "Trait table.\nYou can prepare your trait file following the trait data file The data can be quantitative traits or qualitative traits.\nFor qualitative:\nformat1.\n\n\n\nsample_id\ntype\n\n\n\n\nS_0001\ntreat\n\n\nS_0002\ntreat\n\n\nS_0003\ntreat\n\n\nS_0004\ncontrol\n\n\nS_0005\ncontrol\n\n\nS_0006\ncontrol\n\n\n\nif the input trait data have only 2 columns, the format1 will be automaticly transformed to format2.\nformat2.\n\n\n\nsample_id\ntreat\ncontrol\n\n\n\n\nS_0001\n1\n0\n\n\nS_0002\n1\n0\n\n\nS_0003\n1\n0\n\n\nS_0004\n0\n1\n\n\nS_0005\n0\n1\n\n\nS_0006\n0\n1\n\n\n\nFor quantitative:\n\n\n\nsample_id\nplant_height\nyield\n\n\n\n\nS_0001\n12\n1920\n\n\nS_0002\n14\n1930\n\n\nS_0003\n13\n1919\n\n\nS_0004\n20\n3020\n\n\nS_0005\n25\n3021\n\n\nS_0006\n22\n3320",
    "crumbs": [
      "Data mining",
      "iterative WGCNA"
    ]
  },
  {
    "objectID": "content/iWGCNA.html#step1.-expression-matrix-filtering",
    "href": "content/iWGCNA.html#step1.-expression-matrix-filtering",
    "title": "Data mining by iWGCNA",
    "section": "Step1. Expression matrix filtering",
    "text": "Step1. Expression matrix filtering\nAccording to the WGCNA FAQ, the expression data we input needs to undergo data cleaning before it can be used for WGCNA analysis. We have processed the data as follows:\n\nFor read count values in RNAseq data, we need to normalize them using the vst function from DESeq2. For already normalized count values, such as FPKM, RPKM, TPM values, etc., we can use the original values, or transform them using log10(x+1).\n\n\n“We then recommend a variance-stabilizing transformation. For example, package DESeq2 implements the function varianceStabilizingTransformation which we have found useful, but one could also start with normalized counts (or RPKM/FPKM data) and log-transform them using log2(x+1). For highly expressed features, the differences between full variance stabilization and a simple log transformation are small.”\n\n\nFor noise removal, a specific explanation can be referred to:\n\n\n“We suggest removing features whose counts are consistently low (for example, removing all features that have a count of less than say 10 in more than 90% of the samples) because such low-expressed features tend to reflect noise and correlations based on counts that are mostly zero aren’t really meaningful. The actual thresholds should be based on experimental design, sequencing depth and sample counts.”\n\n\nFilter the top N genes with the greatest variation through median absolute deviation (MAD) or based on variance (VAR) for subsequent analysis.\n\n\n“Probesets or genes may be filtered by mean expression or variance (or their robust analogs such as median and median absolute deviation, MAD) since low-expressed or non-varying genes usually represent noise. Whether it is better to filter by mean expression or variance is a matter of debate; both have advantages and disadvantages, but more importantly, they tend to filter out similar sets of genes since mean and variance are usually related.”\n\nParameters:\nFormat:\n\ncount, integer, read count from RNAseq data\n\nexpected count, float, expected count generated by RSEM,\n\nnormalized count, float, normalized expression levels, such as FPKM, RPKM, or TPM\n\npeak area, float, peak area of metabolites produced by LC-MS. Typically, we perform data cleaning and normalization on the peak area in the analysis results.\n\nprotein abundance, the results from software like PD might be protein abundance, while MaxQuant might provide corrected intensity. Both can be used. However, when the input is corrected intensity, the values are less than 1. In this case, be mindful of the threshold selection in subsequent filtering.\n\nNormalized method\n\nraw use the raw value.\nlogarithm use log10(x+1)\nSample percentage 0-1, In what percentage of samples does the expression level fall below the cutoff\nExpression Cutoff numeric, In what percentage of samples does the expression level fall below the cutoff\n\nFilter Method\n\nMAD median absolute deviation\nSVR variance\n\nReserved genes Num integer, How many genes would you like to retain for WGCNA analysis after the filtering process? Please note, if this number surpasses the count of genes remaining after filtering, all the filtered genes will be preserved.\nStep by step",
    "crumbs": [
      "Data mining",
      "iterative WGCNA"
    ]
  },
  {
    "objectID": "content/iWGCNA.html#step2.-selection-and-validation-of-the-soft-threshold",
    "href": "content/iWGCNA.html#step2.-selection-and-validation-of-the-soft-threshold",
    "title": "Data mining by iWGCNA",
    "section": "Step2. selection and validation of the soft-threshold",
    "text": "Step2. selection and validation of the soft-threshold\nIn the construction of a weighted co-expression network, we need to select an appropriate soft-thresholding value to build a scale-free network. The criteria for judging whether the soft-thresholding value is appropriate generally consider that the closer the mean connectivity is to 0 and the signed R^2 is to 1 under this power value, the closer the network is to a scale-free network. However, the power should not be too large, as a larger power value may lead to a higher false positive rate. Therefore, we usually choose the power value that first crosses the R2 threshold line and has a mean connectivity closest to 0 under this power value as the soft-thresholding value for subsequent analysis.\nParameters:\nR^2 cutoff numeric, cutoff of scale-free topology model fit.\nPower type If the recommended power value provided by the software is suitable, select ‘recommend’. If it is found to be unsuitable, choose an appropriate threshold based on the two graphs on the right, then change this option to ‘customized’, and select the threshold you want to use in ‘final power selection’.\nStep by step",
    "crumbs": [
      "Data mining",
      "iterative WGCNA"
    ]
  },
  {
    "objectID": "content/iWGCNA.html#step3.-one-step-network-construction-and-module-detection.",
    "href": "content/iWGCNA.html#step3.-one-step-network-construction-and-module-detection.",
    "title": "Data mining by iWGCNA",
    "section": "Step3. One step network construction and module detection.",
    "text": "Step3. One step network construction and module detection.\nSteps for scale-free network construction：\nSimilarity Matrix Construction: Calculate the correlation between all genes, usually using the Pearson correlation coefficient. This results in a similarity matrix.\nWeighted Similarity Matrix: Transform the similarity matrix into a weighted similarity matrix. This is achieved by raising each element in the similarity matrix to a positive power β (β&gt;1). The choice of β is to make the network satisfy scale-free distribution.\nTopological Overlap Matrix: Calculate the topological overlap between genes, which reflects how similar the neighbors of two genes are. The purpose of this step is to reduce noise and sparsity.\nGene Module Detection: Use hierarchical clustering methods to cluster the genes in the topological overlap matrix into different modules. These modules are a set of highly co-expressed genes.\nParameters:\nmin Module Size The minimum number of genes to form a module implies that if there are not enough genes, they cannot be divided into a module.\nmodule cuttree height Refers to the threshold set on the hierarchical clustering dendrogram, which determines the partitioning of genes into distinct modules.\nselect max blocksize When calculating correlations, if you input tens of thousands of genes, the calculation requires a large amount of memory. If there is insufficient memory, the calculation will be terminated. At this point, you need to divide the genes into different blocks, calculate separately, divide the modules, and finally merge the modules. Of course, our suggestion is to put as many genes as possible into one block. The table below shows the theoretical relationship between memory size and max block size.\n\n\n\nmemory\nblock size\n\n\n\n\n8G\n5000-10000\n\n\n16G\n10000-20000\n\n\n32G\n20000-30000\n\n\n64G\n30000-40000\n\n\n\nstep by step",
    "crumbs": [
      "Data mining",
      "iterative WGCNA"
    ]
  },
  {
    "objectID": "content/iWGCNA.html#step4.-module-trait-relationship.",
    "href": "content/iWGCNA.html#step4.-module-trait-relationship.",
    "title": "Data mining by iWGCNA",
    "section": "Step4. Module-trait relationship.",
    "text": "Step4. Module-trait relationship.\nAfter dividing genes into different modules, based on the experimental design, we need to see if these modules are associated with the traits we are testing, or with sample classification information. This step can reveal and explore the connection between modules and biological issues, and is an important part of WGCNA analysis.\nAdditionally, we provide a simple method for purifying modules. Inspired by Emily et.al , we also aim to achieve better co-expression network analysis results through iterative WGCNA. However, we did not adopt their method. We simply gradually remove genes with low kME and those that cannot be classified, resulting in a more purified module division.\nTo facilitate the reproduction of previous data analysis processes, or to conduct further analysis in R, we have saved the variables prior to module construction.\nParameters:\n「NOTICE」For iterative WGCNA, Method 1 is not work for now, please use method 2.\nKME cutoff numeric, Remove genes with the kME cutoff of each module.\nChoose method,\nmethod 1: not work now\nmethod 2: recommanded.\nstep by step",
    "crumbs": [
      "Data mining",
      "iterative WGCNA"
    ]
  },
  {
    "objectID": "content/iWGCNA.html#step5.-interested-module-and-hubgene.",
    "href": "content/iWGCNA.html#step5.-interested-module-and-hubgene.",
    "title": "Data mining by iWGCNA",
    "section": "Step5. Interested module and hubgene.",
    "text": "Step5. Interested module and hubgene.\nAfter obtaining the modules of interest through module-trait, we want to see the specific relationship between the module and the traits. In subsequent steps, we can calculate the correlation between genes within the module and the phenotype to get Gene Significance (GS), and calculate the correlation between the gene and the module eigengene to get Module Membership (MM). A higher MM indicates that the gene is in a hub position in the co-expression network constructed by the module, and a high GS indicates that the gene’s expression pattern is highly correlated with the phenotype, and is likely to affect the formation of the trait. In this way, we have found the key genes controlling the phenotype and the gene co-expression network driven by them.\nIf your analysis does not include trait data, you can select the genes with the highest kME values in each module as hubgenes based on the previous kME table. Alternatively, you can group the materials according to the materials, and then try the following steps. However, when determining whether it is a hubgene, you only need to consider MM, not GS, after all, your biological significance is not mainly about sample classification, and the significant relationship between the module and sample classification is not your focus. Your focus should be on the function of each module and the core of the co-expression network. This experimental design suggests that after the module division is completed, perform enrichment analysis for each module, find the module related to the biological issue you are concerned about, and then find the hubgenes of the module via only MM (kME).\nparameters\ncutoff of absoulute value of kME Generally, it is required to be above 0.8. If it’s too low, the module division may not be appropriate.\ncutoff of absoulute value of GS Generally, it is required to be above 0.5.\nweight threshold Generally, it is required to be above 0.02. If the module is large and there are many edge results, the visualization can be quite messy, so you can appropriately increase the weight threshold.\nStep by Step",
    "crumbs": [
      "Data mining",
      "iterative WGCNA"
    ]
  },
  {
    "objectID": "content/pre_targeted.html",
    "href": "content/pre_targeted.html",
    "title": "Pseudotargeted metabolomics",
    "section": "",
    "text": "Pseudotargeted metabolomics"
  },
  {
    "objectID": "content/Upstream_method.html",
    "href": "content/Upstream_method.html",
    "title": "Upsteam Data Process",
    "section": "",
    "text": "This Shiny application is developed based on the tidyMass package, specifically designed for the analysis of non-targeted/targeted metabolomics data. Dr. Xiaotao Shen’s tidyMass is an excellent open-source software tailored for metabolomics data processing. It adheres to the tidyverse development philosophy, significantly enhancing code readability. Additionally, the newly added mass_dataset object makes the metabolomics data analysis process more transparent and reproducible.\n\n\n\n\nlibrary(MetMiner)\nlibrary(tidyverse)\nlibrary(tidymass)\nlibrary(MDAtoolkits)\nrun_metMiner(maxRequestSize = 300)\n\n \n\n\n\nBefore starting the project, we need to set up the working directory and upload sample information. You have the following three ways to upload files. \nStart with Ms file: If your data is in raw format, you need to first convert it to .mgf format using HPC_tidymass. Then click on ‘Start with Ms file’ to upload the file. Then upload the folder containing MS2. \n\nStart with table file: If your data is a post-peak metabolite expression matrix, you can click on the ‘Start with table file’ button to upload the data. The sample table is as follows, and the first four columns must exist, with column names not to be changed. Then upload the folder containing MS2.  \nStart with massdataset object: If your data is generated by tidymass, you can choose the ‘Start with massdataset object’ button to upload your data. Then upload the folder containing MS2.",
    "crumbs": [
      "Upstream data processing",
      "Introduction"
    ]
  },
  {
    "objectID": "content/Upstream_method.html#introduce",
    "href": "content/Upstream_method.html#introduce",
    "title": "Upsteam Data Process",
    "section": "",
    "text": "This Shiny application is developed based on the tidyMass package, specifically designed for the analysis of non-targeted/targeted metabolomics data. Dr. Xiaotao Shen’s tidyMass is an excellent open-source software tailored for metabolomics data processing. It adheres to the tidyverse development philosophy, significantly enhancing code readability. Additionally, the newly added mass_dataset object makes the metabolomics data analysis process more transparent and reproducible.",
    "crumbs": [
      "Upstream data processing",
      "Introduction"
    ]
  },
  {
    "objectID": "content/Upstream_method.html#usage-of-shinyapp",
    "href": "content/Upstream_method.html#usage-of-shinyapp",
    "title": "Upsteam Data Process",
    "section": "",
    "text": "library(MetMiner)\nlibrary(tidyverse)\nlibrary(tidymass)\nlibrary(MDAtoolkits)\nrun_metMiner(maxRequestSize = 300)\n\n \n\n\n\nBefore starting the project, we need to set up the working directory and upload sample information. You have the following three ways to upload files. \nStart with Ms file: If your data is in raw format, you need to first convert it to .mgf format using HPC_tidymass. Then click on ‘Start with Ms file’ to upload the file. Then upload the folder containing MS2. \n\nStart with table file: If your data is a post-peak metabolite expression matrix, you can click on the ‘Start with table file’ button to upload the data. The sample table is as follows, and the first four columns must exist, with column names not to be changed. Then upload the folder containing MS2.  \nStart with massdataset object: If your data is generated by tidymass, you can choose the ‘Start with massdataset object’ button to upload your data. Then upload the folder containing MS2.",
    "crumbs": [
      "Upstream data processing",
      "Introduction"
    ]
  },
  {
    "objectID": "content/installation.html",
    "href": "content/installation.html",
    "title": "Installation",
    "section": "",
    "text": "The MetMiner pipeline is primarily driven by three R packages: TidyMass MDAtoolkits WGCNA, each responsible for different components as illustrated in Figure 1. These packages are integrated into two user-friendly Shiny applications. We offer two installation methods:\n\nFor users proficient in R and with debugging skills, we recommend the first method, which involves installing the R packages individually.\nFor general users, we suggest installing and running the application via the TBtools plugin store.\n\n\n\n\n\n\n\n\n\nFigure 1: Structure of metMiner pipeline.\n\n\n\n\n\n\n\n\nCopy and paste following code into your R script or R console and run:\n\n##&gt; for chinese users:\n##&gt; options(\"repos\" = c(CRAN=\"https://mirrors.tuna.tsinghua.edu.cn/CRAN/\"))\n\nif (!require('remotes')) install.packages('remotes');\nif (!require('Hmisc')) install.packages('Hmisc');\nif (!require('tidymass')) {\n  source(\"https://www.tidymass.org/tidymass-packages/install_tidymass.txt\");\n  install_tidymass(from = \"tidymass.org\")\n};\nif (!require('ropls')) remotes::install_github(\"SamGG/ropls\");\nif (!require('PCAtools')) remotes::install_github('kevinblighe/PCAtools');\nif (!require('MDAtoolkits')) remotes::install_github('ShawnWx2019/MDAtoolkits',ref = 'master');\nif (!require('shinyFiles')) remotes::install_github('thomasp85/shinyFiles');\nif (!require('shinyWidgets')) remotes::install_github(\"dreamRs/shinyWidgets\");\nif (!require('ComplexHeatmap')) remotes::install_github('jokergoo/ComplexHeatmap');\nif (!require('clusterProfiler')) remotes::install_github('YuLab-SMU/clusterProfiler');\nif (!require('shinyjs')) install.packages('shinyjs');\nif (!require('dashboardthemes')) install.packages('dashboardthemes');\nif (!require(\"DT\")) install.packages('DT');\nif (!require('shiny')) install.packages('shiny');\nif (!require('bsicons')) install.packages('bsicons');\nif (!require('bslib')) install.packages('bslib');\nif (!require('ggsci')) install.packages('ggsci');\nif (!require('plotly')) install.packages('plotly');\nif (!require('ggrepel')) install.packages('ggrepel');\nif (!require('shinythemes')) install.packages('shinythemes');\nif (!require('ggstatsplot')) install.packages('ggstatsplot');\nif (!require('patchwork')) install.packages('patchwork');\nif (!require('tidyverse')) install.packages('tidyverse');\nif (!require('shinyjqui')) install.packages('shinyjqui');\nif (!require('colourpicker')) install.packages('colourpicker');\nif (!require('RCurl')) install.packages('RCurl');\nif (!require('MetMiner')) remotes::install_github('ShawnWx2019/MetMiner');\n\nrunning following code to start metMiner shinyapp\n\nlibrary(tidyverse)\nlibrary(tidymass)\nlibrary(MDAtoolkits)\nlibrary(MetMiner)\n##&gt; start metMiner shinyapp\nrun_metMiner(maxRequestSize = 300)\n\n\n\n\nClone github repo to local\n# clone this repo to your machine\ngit clone git@github.com:ShawnWx2019/WGCNA-shinyApp.git WGCNAshiny\n\ncd WGCNAshiny\n\n## Method 1.\n\nRscript WGCNAbyClick.v1.R\n\n## Method 2. open WGCNAbyClick.v1.R by Rstudio or other IDE you perfer and run this script.\nDownload zip file and started from Rstudio\n\n\n\n\n\n\n\n\n\n\n\n(a) Github page\n\n\n\n\n\n\n\n\n\n\n\n(b) Shiny Script\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Run App\n\n\n\n\n\n\n\n\n\n\n\n(d) Web-page\n\n\n\n\n\n\n\nFigure 2: getting started\n\n\n\n\n\n\n\nFollow the following steps to install RServer.plugin, MetMiner shinyapp and WGCNA shinyapp\n\n\n\n\n\n\n\n\n\n\n\n(a) Find plugin store\n\n\n\n\n\n\n\n\n\n\n\n(b) Install Rserver plugin\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Download Rserver plugin\n\n\n\n\n\n\n\n\n\n\n\n(d) Install Rserver plugin\n\n\n\n\n\n\n\nFigure 3: Step by step TBtools plugin install\n\n\n\nInstall MetMiner shiny plugin and WGCNA shiny plugin in same way.\n\n\n\n\n\n\n\n\nstart wgcna shinyapp\n\n\n\n\n\n\n\nstart wgcna shinyapp\n\n\n\n\n\n\nFigure 4: start shinyapps",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "content/installation.html#structure-of-metminer",
    "href": "content/installation.html#structure-of-metminer",
    "title": "Installation",
    "section": "",
    "text": "Figure 1: Structure of metMiner pipeline.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "content/installation.html#method-1-install-from-r-or-rstudio",
    "href": "content/installation.html#method-1-install-from-r-or-rstudio",
    "title": "Installation",
    "section": "",
    "text": "Copy and paste following code into your R script or R console and run:\n\n##&gt; for chinese users:\n##&gt; options(\"repos\" = c(CRAN=\"https://mirrors.tuna.tsinghua.edu.cn/CRAN/\"))\n\nif (!require('remotes')) install.packages('remotes');\nif (!require('Hmisc')) install.packages('Hmisc');\nif (!require('tidymass')) {\n  source(\"https://www.tidymass.org/tidymass-packages/install_tidymass.txt\");\n  install_tidymass(from = \"tidymass.org\")\n};\nif (!require('ropls')) remotes::install_github(\"SamGG/ropls\");\nif (!require('PCAtools')) remotes::install_github('kevinblighe/PCAtools');\nif (!require('MDAtoolkits')) remotes::install_github('ShawnWx2019/MDAtoolkits',ref = 'master');\nif (!require('shinyFiles')) remotes::install_github('thomasp85/shinyFiles');\nif (!require('shinyWidgets')) remotes::install_github(\"dreamRs/shinyWidgets\");\nif (!require('ComplexHeatmap')) remotes::install_github('jokergoo/ComplexHeatmap');\nif (!require('clusterProfiler')) remotes::install_github('YuLab-SMU/clusterProfiler');\nif (!require('shinyjs')) install.packages('shinyjs');\nif (!require('dashboardthemes')) install.packages('dashboardthemes');\nif (!require(\"DT\")) install.packages('DT');\nif (!require('shiny')) install.packages('shiny');\nif (!require('bsicons')) install.packages('bsicons');\nif (!require('bslib')) install.packages('bslib');\nif (!require('ggsci')) install.packages('ggsci');\nif (!require('plotly')) install.packages('plotly');\nif (!require('ggrepel')) install.packages('ggrepel');\nif (!require('shinythemes')) install.packages('shinythemes');\nif (!require('ggstatsplot')) install.packages('ggstatsplot');\nif (!require('patchwork')) install.packages('patchwork');\nif (!require('tidyverse')) install.packages('tidyverse');\nif (!require('shinyjqui')) install.packages('shinyjqui');\nif (!require('colourpicker')) install.packages('colourpicker');\nif (!require('RCurl')) install.packages('RCurl');\nif (!require('MetMiner')) remotes::install_github('ShawnWx2019/MetMiner');\n\nrunning following code to start metMiner shinyapp\n\nlibrary(tidyverse)\nlibrary(tidymass)\nlibrary(MDAtoolkits)\nlibrary(MetMiner)\n##&gt; start metMiner shinyapp\nrun_metMiner(maxRequestSize = 300)\n\n\n\n\nClone github repo to local\n# clone this repo to your machine\ngit clone git@github.com:ShawnWx2019/WGCNA-shinyApp.git WGCNAshiny\n\ncd WGCNAshiny\n\n## Method 1.\n\nRscript WGCNAbyClick.v1.R\n\n## Method 2. open WGCNAbyClick.v1.R by Rstudio or other IDE you perfer and run this script.\nDownload zip file and started from Rstudio\n\n\n\n\n\n\n\n\n\n\n\n(a) Github page\n\n\n\n\n\n\n\n\n\n\n\n(b) Shiny Script\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Run App\n\n\n\n\n\n\n\n\n\n\n\n(d) Web-page\n\n\n\n\n\n\n\nFigure 2: getting started",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "content/installation.html#method-2-install-from-tbtools-plugin-store",
    "href": "content/installation.html#method-2-install-from-tbtools-plugin-store",
    "title": "Installation",
    "section": "",
    "text": "Follow the following steps to install RServer.plugin, MetMiner shinyapp and WGCNA shinyapp\n\n\n\n\n\n\n\n\n\n\n\n(a) Find plugin store\n\n\n\n\n\n\n\n\n\n\n\n(b) Install Rserver plugin\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Download Rserver plugin\n\n\n\n\n\n\n\n\n\n\n\n(d) Install Rserver plugin\n\n\n\n\n\n\n\nFigure 3: Step by step TBtools plugin install\n\n\n\nInstall MetMiner shiny plugin and WGCNA shiny plugin in same way.\n\n\n\n\n\n\n\n\nstart wgcna shinyapp\n\n\n\n\n\n\n\nstart wgcna shinyapp\n\n\n\n\n\n\nFigure 4: start shinyapps",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "content/Import_data.html",
    "href": "content/Import_data.html",
    "title": "Data import",
    "section": "",
    "text": "Data import\n\nUpload MS file\nClick the dropdown button Import file and upload MS1, MS/MS files, as shown in the structure below. Click the 1. Check input file button to check the data. After confirming correctness, click the 2. Peak picking button for peak picking. This is a relatively lengthy process, and the progress bar will be displayed in the bottom right corner.\n\n\n\nData cleaning\nData cleaning includes: Overview, Remove noisy features, Remove outliers, Missing value imputation, and Normalization.\n\n\nOverview\nFirst, upload sample information and set parameters, then Update sample information, click Start, and finally open the Interactive plot button to check for missing sample data.\n\nOpen the interactive button, then hover the mouse over the image to see detailed information about the samples in the detailed image.\n\n\n\nRemove noisy features\nFirst, adjust the parameters for grouping, samples, and QC missing frequency on the left side, click Find noisy features to identify noisy features, and then click Remove and update to update the data.\n\n\n\nRemove outliers\nFirst, adjust the parameters on the left side, click Find outliers to identify outliers, open the Interactive plot, hover the mouse over the outlier sample to view outlier sample information, remove outlier values. Then switch to negative spectrum and repeat the operation, then click Remove and update to update the data.\n\n\n\nMissing value imputation\nSelect the method for missing value imputation, such as: knn, rf, mean, median, zero, mininum, bpca, svdlmpute, ppca, etc., click Start to proceed with imputation.\n\n\n\nData standardization\nFirst, adjust the method for filling missing values on the left side, such as svr, total, mean,median, pqn, loess ppca, etc., click Start normalization to standardize the data. Select PCA color grouping. Then click Visualize to visualize the data before and after normalization. Open the Interactive plot button, and you can see the 3D PCA plot. Hovering the mouse over it allows you to see detailed information about the samples. Finally, click the Export normalized data button to download the normalized data.",
    "crumbs": [
      "Upstream data processing",
      "Data import"
    ]
  },
  {
    "objectID": "content/dam.html",
    "href": "content/dam.html",
    "title": "Different Accumulated Metabolite Analysis",
    "section": "",
    "text": "Different Accumulated Metabolite Analysis\nFirst, we set up the comparison groups and sample IDs. Finally, we adjust the Parameters for DAM, such as Logarithmic transformation, p-value, q-value, VIP, log2fc, Univariate test method, Paired, Multivariate test method, p adjust method, and so on. Click the Check your compare group button to proceed with the Different Accumulated Metabolite Analysis. If you need to start the analysis from the ‘Resuming analysis from the unfinished steps (options)’, click Wake up first to activate the mass_dataset. Switch tabs to view the results of the differential analysis, such as the Upset plot, PCA, PLS-DA, OPLS-DA, Volcano plot and boxplot. Open the interactive plot button and click on the metabolites in the Volcano plot to view detailed information such as Compound annotation, MS2 spectra, and Structure.",
    "crumbs": [
      "Downstream data processing",
      "DAM analysis"
    ]
  },
  {
    "objectID": "content/feature_anno.html",
    "href": "content/feature_anno.html",
    "title": "Feature Annotation",
    "section": "",
    "text": "Feature Annotation\nWe have built-in some publicly available MS1 and MS/MS databases (MoNA, Massbank, ReSpect, PlaSMA, MetabOBASE, KEGG, KNApSAcK, Ath_Cyc, Orbitrap, Zma_Cyc), which can be selected in the parameters column on the left side. You can also use Tidymass to build your own database, which needs to be in .rda format. After setting the parameters, click Start annotation.",
    "crumbs": [
      "Downstream data processing",
      "Feature annotation"
    ]
  },
  {
    "objectID": "content/summary.html",
    "href": "content/summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "content/Project_init.html",
    "href": "content/Project_init.html",
    "title": "Project initiate",
    "section": "",
    "text": "Project initiate\n\nSet working directory.\nUpload sample information file (must be a .csv file).\nIf there are changes in column names when preparing sample information, you can click the dropdown button to correspond to them.\nCheck sample information.\n\nIf you have unfinished analyses,you need to upload the previously generated mass_dataset (.rda) files (both positive and negative model), and then select the steps you wish to continue with. The optional steps include Removing noisy features, Removing outliers, Imputing missing values, Normalization, Annotation,Annotation filtering,Data integrate and DAM and rest.",
    "crumbs": [
      "Upstream data processing",
      "Project initiate"
    ]
  },
  {
    "objectID": "content/feature_class.html",
    "href": "content/feature_class.html",
    "title": "Classification",
    "section": "",
    "text": "Classification\nFirst, you need to prepare a table like this, set the parameters, then upload the file, and click the Start button. Click the show pie plot button, switch to the plot tab, and you can see the classification results.\n\n\n\nvariable_id\nCompound.name\nLab.ID\n\n\n\n\nM105T63_NEG\nGlyceric acid\nMONA_15722\n\n\nM111T107_NEG\n2-Furancarboxylic acid\nNO07297\n\n\nM113T918_NEG\nAcetylenedicarboxylate\nC03248\n\n\nM115T64_NEG\nMaleic acid\nMONA_15797\n\n\nM115T96_NEG\n3-Methyl-2-oxobutanoic acid\nC00141\n\n\nM117T75_NEG\nSuccinic acid\nNO07371",
    "crumbs": [
      "Downstream data processing",
      "Classification"
    ]
  }
]